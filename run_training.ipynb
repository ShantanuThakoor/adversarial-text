{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dataset = namedtuple('Dataset', ['x', 'y', 'vocab', 'lm'])\n",
    "import linear\n",
    "import kenlm\n",
    "import solver\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "reload(solver)\n",
    "Printer = solver.Printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_all(models, names, dataset_word, dataset_char, n, results, lm_loss_limit=2.0, **kwargs):\n",
    "    for name in names:\n",
    "        solver.lm = dataset_word.lm\n",
    "        dataset = dataset_word\n",
    "        if name.startswith('VDCNN'):\n",
    "            dataset = dataset_char\n",
    "        if name.startswith('NB'):\n",
    "            results[name] = linear.main(models[name].clf, yelp_data, n, lm_loss_limit=lm_loss_limit, **kwargs)\n",
    "        else:\n",
    "            results[name] = solver.main(models[name], dataset.x[0:n], dataset.y[0:n], dataset.vocab, \n",
    "                                                    lm_loss_limits=(lm_loss_limit, ), latex=False, printer=solver.NullPrinter(), **kwargs)\n",
    "    return results\n",
    "def show_lm(data, results, n_samples=5, key='original_tokens'):    \n",
    "    lm_results = np.array([solver.score_paragraph(data.lm, entry[key]) for entry in results if entry['score'] > 0.5])\n",
    "    pl = lm_results[:, 0] / lm_results[:, 1]\n",
    "    indices = np.nonzero(pl > -1)[0]\n",
    "    np.random.shuffle(indices)\n",
    "    for i in indices[:n_samples]:\n",
    "        print('Sample', i, pl[i], ' '.join(results[i][key]))\n",
    "    plt.hist(pl, bins=20)\n",
    "def generate_random(model, data, n, lm_loss_limits, **kwargs):\n",
    "    solver.lm = data.lm\n",
    "    return solver.main(model, data.x[:n], data.y[:n], data.vocab, lm_loss_limits, printer=solver.NullPrinter(), method='random', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "def full_scores(lm, s, printer):\n",
    "    for sent in nltk.sent_tokenize(s):\n",
    "        tokens = ['<s>'] + filter(lambda x : not is_punct(x), nltk.word_tokenize(sent) ) + ['</s>']\n",
    "        printer.print(sent)\n",
    "        for i, (score, ngram, oov) in enumerate(lm.full_scores(' '.join(tokens[1:-1]), eos=True, bos=True)):\n",
    "            printer.print('{:.2f} {:.2f} {} {}'.format(score, score - lm.score(tokens[i+1], bos=False, eos=False), ' '.join(tokens[i+2-ngram: i+2]), ngram) )\n",
    "\n",
    "def encode_html(s):\n",
    "    return s.replace('<', '&lt;').replace('>', '&gt;')\n",
    "\n",
    "def is_punct(s):\n",
    "    for ch in s.lower():\n",
    "        i = ord(ch)\n",
    "        if 97 <= i <= 122 or 48 <= i <= 57:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def parse_tokens_lm(tokens):\n",
    "    parsed_tokens = [ token.lower() for token in tokens if not is_punct(token) ]\n",
    "    return ' '.join(parsed_tokens)\n",
    "def full_classify(clf, tokens, vocab, p):\n",
    "    if not isinstance(tokens[0], str):\n",
    "        tokens = [ vocab.vocabulary_.reverse(idx) for idx in np.trim_zeros(tokens) ]\n",
    "    delta_prob = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
    "    for token in tokens:\n",
    "        word_idx = vocab.vocabulary_.get(token)\n",
    "        p.print('{} ({:.2f})'.format(encode_html(token), delta_prob[word_idx]), end = ' ')\n",
    "def col(results, key):\n",
    "    return [ result[key] for result in results ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "reload(solver)\n",
    "def score_all(data, results_dict):\n",
    "    def ppl(x):\n",
    "        return 1 / 10 ** x\n",
    "    for k, results in sorted(results_dict.iteritems()):\n",
    "        lm_results = [solver.score_paragraph(data.lm, entry['tokens']) for entry in results if entry['score'] > 0.5]\n",
    "        diffs = [ entry['diff'] / (len(entry['tokens']) + 1e-6) for entry in results if entry['score'] > 0.5]\n",
    "        lm_results = np.array(lm_results)\n",
    "        print(k, 1.0 * len(lm_results) / len(results) )\n",
    "        if len(lm_results) > 0:\n",
    "            print( np.mean(diffs), ppl(np.sum(lm_results[:, 0]) / np.sum(lm_results[:, 1]) ) )\n",
    "        \n",
    "    lm_results = [solver.score_paragraph(data.lm, entry['original_tokens']) for entry in results]\n",
    "    lm_results = np.array(lm_results)\n",
    "    print('Clean')\n",
    "    print( ppl(np.sum(lm_results[:, 0]) / np.sum(lm_results[:, 1]) ) )\n",
    "    print(len(lm_results) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "First run scripts in shell to train the models\n",
    "```sh\n",
    "python word_model.py train --dataset yelp_review_polarity_csv --num_filters 512 --decay 2e-4 --tag lstm --gpu 1 --mem 0.5\n",
    "python word_model.py train --dataset yelp_review_polarity_csv --num_filters 300 --decay 2e-4 --tag conv --gpu 1 --mem 0.5\n",
    "python deep_model.py train --dataset yelp_review_polarity_csv --blocks 1,1,1,1 -v 2-layer9 --gpu 1 --mem 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU = '1' \n",
    "#GPU = '\"\"' # CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from word/runs_yrpc/lstm/model-7876\n",
      "Prepare for run in notebook\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'word/runs_yrpc/conv/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/atlas/u/tflau/Very-Deep-Convolutional-Networks-for-Natural-Language-Processing-in-tensorflow/word_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'notebook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/config.json'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'word/runs_yrpc/conv/config.json'"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'runs_yrpc/v2-layer9/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/atlas/u/tflau/Very-Deep-Convolutional-Networks-for-Natural-Language-Processing-in-tensorflow/deep_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'notebook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/config.json'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'checkpoint'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'runs_yrpc/v2-layer9/config.json'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset yelp_review_polarity_csv loaded ..\n",
      "0.878947368421\n"
     ]
    }
   ],
   "source": [
    "yelp_lm = kenlm.LanguageModel('dataset/yelp_review_polarity_csv/lm.arpa.bin')\n",
    "yelp_models = dict()\n",
    "%run word_model.py notebook --dataset yelp_review_polarity_csv --num_filters 512 --decay 2e-4 --tag lstm --gpu {GPU} --mem 0.5\n",
    "yelp_models['LSTM'] = model\n",
    "yelp_data = dataset(x=x_shuffled, y=y_shuffled, vocab=vocab, lm=yelp_lm)\n",
    "%run word_model.py notebook --dataset yelp_review_polarity_csv --num_filters 300 --decay 2e-4 --tag conv --gpu {GPU} --mem 0.5\n",
    "yelp_models['WordCNN'] = model\n",
    "%run deep_model.py notebook --dataset yelp_review_polarity_csv --blocks 1,1,1,1 -v 2-layer9 --gpu {GPU} --mem 0.5\n",
    "yelp_models['VDCNN-11'] = model\n",
    "yelp_data_char = dataset(x=x_shuffled, y=y_shuffled, vocab=solver.alphabet, lm=yelp_lm)\n",
    "\n",
    "yelp_nb_data = linear.load_dataset('yelp_review_polarity_csv')\n",
    "yelp_clf = linear.model(*yelp_nb_data)\n",
    "yelp_models['NB'] = solver.SklearnAdaptor(yelp_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_results=dict()\n",
    "n = 10\n",
    "_ = generate_all(yelp_models, ['NB', 'VDCNN-11', 'WordCNN', 'LSTM'], yelp_data, yelp_data_char, n=n, results=yelp_results, \n",
    "             target_diffs=0.5, \n",
    "             lm_loss_limit=2.0,\n",
    "             target_proba=(0.9, ) )\n",
    "n_display = 10\n",
    "solver.show_results(yelp_results, (0.9, ), fraction_words=1.00, n = n_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trec_models = dict()\n",
    "%run word_model.py test --tag lstm-mean --dropout 0.5,0.5 --num_filters 300 --dataset trec07p --gpu 1 --mem 0.4\n",
    "trec_lm = kenlm.LanguageModel('lm/trec07p_train.arpa.bin')\n",
    "trec_data = dataset(x=x_shuffled, y=y_shuffled, vocab=vocab, lm=trec_lm)\n",
    "trec_models['LSTM'] = model\n",
    "%run word_model.py test --tag conv300x1 --num_filters 300 --dropout 0.5 --dataset trec07p --gpu 1 --mem 0.4\n",
    "trec_models['WordCNN'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon_models=dict()\n",
    "%run word_model.py notebook --dataset amazon_review_polarity_csv --num_filters 300 --decay 2e-4 --tag conv300x1_dc2e4_l200 --gpu 1 --mem 0.25 --checkpoint 32000 \n",
    "amazon_models['WordCNN'] = model\n",
    "%run word_model.py notebook --dataset amazon_review_polarity_csv --num_filters 512 --decay 2e-4 --tag lstm-mean --gpu 1 --mem 0.25\n",
    "amazon_models['LSTM'] = model\n",
    "amazon_data = dataset(x=x_shuffled, y=y_shuffled, vocab=vocab, lm=kenlm.LanguageModel('lm/arpc_train.arpa.bin'))\n",
    "%run deep_model.py notebook --gpu 1 --dataset amazon_review_polarity_csv --mem 0.3 -v 2\n",
    "amazon_data_char = dataset(x=x_shuffled, y=y_shuffled, vocab=solver.alphabet, lm=amazon_data.lm)\n",
    "amazon_models['VDCNN'] = model\n",
    "amazon_nb_data = linear.load_dataset('amazon_review_polarity_csv')\n",
    "amazon_clf = linear.model(*amazon_nb_data)\n",
    "amazon_models['NB'] = solver.SklearnAdaptor(amazon_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_models = dict()\n",
    "%run word_model.py notebook --dataset aclImdb --num_filters 512 --decay 2e-4 --tag lstm-mean --gpu 1 --mem 0.3 --checkpoint 352\n",
    "imdb_models['LSTM'] = model\n",
    "imdb_data = dataset(x=x_shuffled, y=y_shuffled, vocab=vocab, lm=kenlm.LanguageModel('lm/aclImdb_train.arpa.bin'))\n",
    "%run word_model.py notebook --dataset aclImdb --num_filters 300 --decay 2e-4 --tag conv300x1_dc2e4_l200 --gpu 1 --mem 0.25 --checkpoint 704 \n",
    "imdb_models['WordCNN'] = model\n",
    "imdb_nb_data = linear.load_dataset('aclImdb')\n",
    "imdb_clf = linear.model(*imdb_nb_data)\n",
    "imdb_models['NB'] = solver.SklearnAdaptor(imdb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_lm = kenlm.LanguageModel('lm/yrpc_train.arpa.bin')\n",
    "yelp_models = dict()\n",
    "dataset = namedtuple('Dataset', ['x', 'y', 'vocab', 'lm'])\n",
    "#%run word_model.py notebook --dataset yelp_review_polarity_csv --num_filters 512 --decay 2e-4 --tag lstm-mean --gpu 1 --mem 0.3 --checkpoint 7876\n",
    "#yelp_models['LSTM-I'] = model\n",
    "%run word_model.py notebook --dataset yelp_review_polarity_csv --num_filters 512 --decay 2e-4 --tag lstm-mean-2 --gpu {GPU} --mem 0.3 --checkpoint 7876\n",
    "yelp_models['LSTM'] = model\n",
    "yelp_data = dataset(x=x_shuffled, y=y_shuffled, vocab=vocab, lm=yelp_lm)\n",
    "%run word_model.py notebook --dataset yelp_review_polarity_csv --num_filters 300 --decay 2e-4 --tag conv300x1_dc2e4_l200 --gpu {GPU} --mem 0.25 --checkpoint 7876 \n",
    "yelp_models['WordCNN'] = model\n",
    "%run deep_model.py notebook --gpu {GPU} --dataset yelp_review_polarity_csv --mem 0.3\n",
    "yelp_models['VDCNN-19'] = model\n",
    "yelp_data_char = dataset(x=x_shuffled, y=y_shuffled, vocab=solver.alphabet, lm=yelp_lm)\n",
    "#%run deep_model.py notebook --gpu 1 --dataset yelp_review_polarity_csv --mem 0.4 --blocks 1,1,1,1 -v 2-layer9 --checkpoint 36000\n",
    "#yelp_models['VDCNN-11-I'] = model\n",
    "%run deep_model.py notebook --gpu {GPU} --dataset yelp_review_polarity_csv --mem 0.4 --blocks 1,1,1,1 -v 2-layer9-run2 --checkpoint 27566\n",
    "yelp_models['VDCNN-11'] = model\n",
    "\n",
    "yelp_nb_data = linear.load_dataset('yelp_review_polarity_csv')\n",
    "yelp_clf = linear.model(*yelp_nb_data)\n",
    "yelp_models['NB'] = solver.SklearnAdaptor(yelp_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
