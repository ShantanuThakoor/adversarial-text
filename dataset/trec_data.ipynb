{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "class EmailParser:\n",
    "    def __init__(self):\n",
    "        self.reg_empty_line = re.compile('\\n\\n')\n",
    "        self.reg_subject = re.compile('^Subject: (.*)$')\n",
    "        self.reg_mime_tag = re.compile(r'^-+([\\w\\.=]+)-*$')\n",
    "        self.reg_content_type = re.compile(r'^Content-Type: ([a-z/]+);?$')\n",
    "        self.reg_charset = re.compile(r'charset=(\\S*)')\n",
    "        self.reg_html = re.compile('<.*?>', re.DOTALL)\n",
    "    def __call__(self, lines):\n",
    "        is_content = False\n",
    "        mime_tag = None\n",
    "        mime_opening = 0\n",
    "        content = \"\"\n",
    "        content_type = \"\"\n",
    "        subject = \"\"\n",
    "        find_content_type = 0\n",
    "        lines = [ line.strip() for line in lines ]\n",
    "        charset = \"ascii\"\n",
    "        for i, line in enumerate(lines):\n",
    "            if is_content:\n",
    "#                print(line.strip(), mime_tag, mime_opening)\n",
    "                if re.match(self.reg_mime_tag, line):\n",
    "                    tag = re.match(self.reg_mime_tag, line).group(1)\n",
    "                    mime_tag = tag\n",
    "                    mime_opening = 1\n",
    "                elif re.match(self.reg_content_type, line):\n",
    "                    content_type = re.match(self.reg_content_type, line).group(1)\n",
    "                    #content += content_type + ' '\n",
    "                    find_content_type = 2\n",
    "                elif line.startswith('This is a multi-part message in MIME format'):\n",
    "                    pass\n",
    "                else:\n",
    "                    if line == '':\n",
    "                        mime_opening = 0\n",
    "                    elif mime_opening == 0 and re.match(r'^image', content_type) is None:\n",
    "                        content += line + '\\n' \n",
    "                        \n",
    "                if find_content_type > 0 and re.search(self.reg_charset, line):\n",
    "                    charset = re.search(self.reg_charset, line).group(1)\n",
    "                    #print(lines[i-3:i+1], charset)\n",
    "                find_content_type = max(find_content_type - 1, 0)\n",
    "                \n",
    "            if re.match(self.reg_lines, line):\n",
    "                is_content = True\n",
    "            elif re.match(self.reg_subject, line):\n",
    "                subject = re.match(self.reg_subject, line).group(1).strip()\n",
    "        content = re.sub(self.reg_html, '', content)\n",
    "        try:\n",
    "            return subject.decode(charset), content.decode(charset)\n",
    "        except LookupError as e:\n",
    "            charset='windows-1252'\n",
    "        except UnicodeDecodeError as e:\n",
    "            charset='windows-1252'\n",
    "        return subject.decode(charset, 'ignore'), content.decode(charset, 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#content_types = defaultdict(lambda : 0)\n",
    "def decode(s, charset):\n",
    "    try:\n",
    "        return s.decode(charset)\n",
    "    except (UnicodeDecodeError, LookupError) as e:\n",
    "        re_charset = re.compile(r'charset=\"(\\S+?)\"', re.MULTILINE)\n",
    "        charset = re.search(re_charset, s)\n",
    "        if charset:\n",
    "            charset = charset.group(1)\n",
    "            try:\n",
    "                return s.decode(charset)\n",
    "            except (UnicodeDecodeError, LookupError) as e:\n",
    "                pass\n",
    "        return s.decode('windows-1252', 'ignore')\n",
    "    \n",
    "def fix_line_break(texts, vocab):\n",
    "    linebreak = re.compile(r'(\\w*)=\\n(\\w*)')\n",
    "    boundary = 0\n",
    "    ret = \"\"\n",
    "    for match in linebreak.finditer(texts):\n",
    "        word = match.group(1) + match.group(2)\n",
    "        ret += texts[boundary:match.start(0)]\n",
    "        if match.group(1) != '' and match.group(2) != '' and word in vocab:\n",
    "            ret += word\n",
    "        else:\n",
    "            ret += match.group(1) + ' ' + match.group(2)\n",
    "        boundary = match.end(0)\n",
    "    ret += texts[boundary:]\n",
    "    return ret\n",
    "\n",
    "def fix_coding(texts):\n",
    "    def substitute(m):\n",
    "        return chr(int(m.group(1), 16))\n",
    "    return re.sub(re.compile(r'=([A-Fa-f0-9]{2})=?'), substitute, texts)\n",
    "\n",
    "def parse(texts):\n",
    "    texts = fix_coding(texts)\n",
    "    p = texts.find('\\n\\n')\n",
    "    body = texts[p + 2 : ]\n",
    "    re_mime_id = re.compile(r'--\\S{10,}\\n(\\n|(.*?)\\n\\n)', re.DOTALL)\n",
    "    re_content_type = re.compile(r'Content-Type: ([a-z/]+)')\n",
    "    re_charset = re.compile(r'charset=(\\S+)$', re.MULTILINE)\n",
    "    re_text_type = re.compile(r'text|message')\n",
    "    re_html = re.compile('<.*?>', re.DOTALL)\n",
    "    re_subject = re.compile('^Subject: (.*)$', re.MULTILINE)\n",
    "    \n",
    "    subject = re.search(re_subject, texts[ : p])\n",
    "    if subject:\n",
    "        subject = subject.group(1)\n",
    "    else:\n",
    "        subject = \"\"\n",
    "    \n",
    "    p = 0\n",
    "    parsed = u\"\"\n",
    "    charset = 'ascii'\n",
    "    content_type = \"text\"\n",
    "    for matcher in re_mime_id.finditer(body):\n",
    "        target = body[p : matcher.start(0)]\n",
    "        if re.search(re_text_type, content_type):\n",
    "            parsed += decode(target, charset)\n",
    "        p = matcher.end(0)\n",
    "        if matcher.group(2) and re.search(re_content_type, matcher.group(2)):\n",
    "            content_type = re.search(re_content_type, matcher.group(2)).group(1)\n",
    "            content_types[content_type] += 1\n",
    "        else:\n",
    "            content_type = \"text\"\n",
    "        if matcher.group(2) and re.search(re_charset, matcher.group(2)):\n",
    "            charset = re.search(re_charset, matcher.group(2)).group(1)\n",
    "        else:\n",
    "            charset = 'ascii'\n",
    "    target = body[p : ]\n",
    "    if re.search(re_text_type, content_type):\n",
    "        parsed += decode(target, charset)\n",
    "    parsed = re.sub(re_html, '', parsed)\n",
    "    return subject.decode('windows-1252', 'ignore') + u' ' + parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parsed_dataset = dict()\n",
    "    for k in dataset:\n",
    "        texts = dataset[k]['text']\n",
    "        parsed_dataset[k] = {\n",
    "            'text' : parse(texts),\n",
    "            'label' : dataset[k]['label']\n",
    "        } \n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def parse_dataset(parser, sample=0):\n",
    "    file_id = re.compile(r'inmail\\.(\\d+)')\n",
    "    labels = dict()\n",
    "    with open('full/index') as f:\n",
    "        for line in f.readlines():\n",
    "            label, member = line.split()\n",
    "            labels[ int(re.search(file_id, member).group(1)) ] = label \n",
    "    datasets = dict()\n",
    "    n = 0\n",
    "    with tarfile.open('trec07p.tgz') as files:\n",
    "        for member in tqdm_notebook(files):\n",
    "            m = re.search(file_id, member.name)\n",
    "            if m:\n",
    "                fid = int(m.group(1))\n",
    "                label = labels[fid]\n",
    "                f = files.extractfile(member)\n",
    "                datasets[fid] = {\n",
    "                    'text' : f.read(),\n",
    "                    'label' : label\n",
    "                }\n",
    "                f.close()\n",
    "                n += 1\n",
    "                if sample and n > sample:\n",
    "                    break\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir trec07p\n",
    "dataset = parse_dataset(None, sample=0)\n",
    "parsed_dataset = main()\n",
    "# with open('trec07p/full_encoded.json', 'w') as fo:\n",
    "#     json.dump(parsed_dataset, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "freq = defaultdict(lambda : 0)\n",
    "for i in tqdm_notebook(range(1, 10000)):\n",
    "    for token in nltk.word_tokenize(parsed_dataset[unicode(i)]['text']):\n",
    "        freq[token.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "# with open('full_encoded.json', 'r') as f:\n",
    "#     parsed_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_line = re.compile(r'\\s*\\n\\s*')\n",
    "white_space = re.compile(r'[^\\S\\n]+')\n",
    "css_re = re.compile(r'.*{\\s*\\S+\\s*:[\\s\\S]*}')\n",
    "space_re = re.compile(r'&nbsp;')\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = fix_line_break(text, freq)\n",
    "    text = re.sub(css_re, ' ', text)\n",
    "    text = re.sub(space_re, ' ', text)\n",
    "    text = re.sub(new_line, '\\n', text)\n",
    "    text = re.sub(white_space, ' ', text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "labels = []\n",
    "for k in tqdm_notebook(range(1, len(parsed_dataset) + 1) ):\n",
    "    text = parsed_dataset[unicode(k)]['text']\n",
    "    text = clean_text(text)\n",
    "    clean_data.append(text)\n",
    "    labels.append(parsed_dataset[unicode(k)]['label'])    \n",
    "clean_dataset = {'texts' : clean_data, \"labels\" : labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('full_clean.csv', 'w') as fo:\n",
    "#     json.dump({'texts' : clean_data, \"labels\" : labels } , fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('full_clean.csv', 'r') as f:\n",
    "#     dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "\n",
    "def split_dataset(dataset, seed=233, shuffle=True):\n",
    "    subsets = [ ('train', 0.9), ('test', 0.1) ]\n",
    "    k = next(dataset.iterkeys())\n",
    "    n = len(dataset[k])\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(n)\n",
    "    else:\n",
    "        indices = range(n)\n",
    "    s = 0\n",
    "    \n",
    "    for subset_name, fraction in subsets:\n",
    "        m = int(n * fraction)\n",
    "        print(subset_name, m)\n",
    "        sub_indices = indices[s : s + m]\n",
    "        print(subset_name, s, s + m)\n",
    "        s += m\n",
    "        subset = dict()\n",
    "        for k in dataset:\n",
    "            subset[k] = [dataset[k][i]  for i in sub_indices ] \n",
    "\n",
    "        with open('trec07p/{}.json'.format(subset_name), 'w') as fo:\n",
    "            json.dump(subset, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train', 56017)\n",
      "('train', 0, 56017)\n",
      "('test', 6224)\n",
      "('test', 56017, 62241)\n"
     ]
    }
   ],
   "source": [
    "split_dataset(clean_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_dataset(dataset):\n",
    "    stext = sorted(range(len(dataset['texts'])),key=lambda x : dataset['texts'][x])\n",
    "    select_indices = []\n",
    "    for k, index_iter in itertools.groupby(stext, key=lambda x : dataset['texts'][x]):\n",
    "        select_indices.append(next(index_iter))\n",
    "    unique_dataset = dict()\n",
    "    for k in dataset:\n",
    "        unique_dataset[k] = [ dataset[k][i] for i in select_indices ]\n",
    "    return unique_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "def further_clean(texts, n=0):\n",
    "    #css_re = re.compile(r'.*{[\\s\\S]*:[\\s\\S]*}')\n",
    "    #css_re = re.compile(r'this is a multi-part message in mime format.|.*{\\s*\\S+\\s*:[\\s\\S]*}')\n",
    "    css_re = re.compile(r'.*{\\s*\\S+\\s*:[\\s\\S]*}')\n",
    "    space_re = re.compile(r'&nbsp;')\n",
    "    return [re.sub(space_re, ' ', re.sub(css_re, '', text) ) for text in tqdm_notebook(texts)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
